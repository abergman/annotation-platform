"""
Comprehensive Test Suite for Batch Operations

Tests for batch annotation operations, import/export, validation,
and performance with various scenarios and edge cases.
"""

import pytest
import asyncio
import json
import csv
import io
import tempfile
from datetime import datetime, timedelta
from uuid import uuid4
from unittest.mock import Mock, patch, AsyncMock

from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine
from fastapi.testclient import TestClient

from src.core.database import Base, get_db
from src.models.user import User
from src.models.project import Project
from src.models.text import Text
from src.models.label import Label
from src.models.annotation import Annotation\nfrom src.models.batch_models import BatchOperation, BatchProgress, BatchError\nfrom src.utils.batch_processor import BatchProcessor, BatchResult\nfrom src.utils.progress_tracker import ProgressTracker\nfrom src.utils.validation_engine import ValidationEngine, ValidationResult\nfrom src.utils.batch_import_export import BatchImportExport\nfrom src.api.batch import router as batch_router\nfrom src.main import app


@pytest.fixture\ndef engine():\n    \"\"\"Create in-memory SQLite database for testing.\"\"\"\n    engine = create_engine(\"sqlite:///:memory:\", echo=False)\n    Base.metadata.create_all(engine)\n    return engine\n\n\n@pytest.fixture\ndef db_session(engine):\n    \"\"\"Create database session for testing.\"\"\"\n    Session = sessionmaker(bind=engine)\n    session = Session()\n    yield session\n    session.close()\n\n\n@pytest.fixture\ndef test_user(db_session):\n    \"\"\"Create test user.\"\"\"\n    user = User(\n        username=\"testuser\",\n        email=\"test@example.com\",\n        hashed_password=\"hashed_password\",\n        full_name=\"Test User\",\n        role=\"researcher\",\n        is_active=True\n    )\n    db_session.add(user)\n    db_session.commit()\n    return user\n\n\n@pytest.fixture\ndef test_admin(db_session):\n    \"\"\"Create test admin user.\"\"\"\n    admin = User(\n        username=\"admin\",\n        email=\"admin@example.com\",\n        hashed_password=\"hashed_password\",\n        full_name=\"Admin User\",\n        role=\"admin\",\n        is_admin=True,\n        is_active=True\n    )\n    db_session.add(admin)\n    db_session.commit()\n    return admin\n\n\n@pytest.fixture\ndef test_project(db_session, test_user):\n    \"\"\"Create test project.\"\"\"\n    project = Project(\n        name=\"Test Project\",\n        description=\"Test project for batch operations\",\n        owner_id=test_user.id,\n        is_active=True\n    )\n    db_session.add(project)\n    db_session.commit()\n    return project\n\n\n@pytest.fixture\ndef test_labels(db_session, test_project):\n    \"\"\"Create test labels.\"\"\"\n    labels = [\n        Label(\n            name=\"PERSON\",\n            description=\"Person entity\",\n            color=\"#ff0000\",\n            project_id=test_project.id\n        ),\n        Label(\n            name=\"LOCATION\",\n            description=\"Location entity\",\n            color=\"#00ff00\",\n            project_id=test_project.id\n        ),\n        Label(\n            name=\"ORGANIZATION\",\n            description=\"Organization entity\",\n            color=\"#0000ff\",\n            project_id=test_project.id\n        )\n    ]\n    \n    for label in labels:\n        db_session.add(label)\n    db_session.commit()\n    return labels\n\n\n@pytest.fixture\ndef test_texts(db_session, test_project):\n    \"\"\"Create test texts.\"\"\"\n    texts = [\n        Text(\n            title=\"Test Document 1\",\n            content=\"John Smith works at Microsoft in Seattle. He is a software engineer.\",\n            project_id=test_project.id,\n            word_count=12,\n            character_count=69\n        ),\n        Text(\n            title=\"Test Document 2\",\n            content=\"Apple Inc. is headquartered in Cupertino, California. Tim Cook is the CEO.\",\n            project_id=test_project.id,\n            word_count=13,\n            character_count=76\n        )\n    ]\n    \n    for text in texts:\n        db_session.add(text)\n    db_session.commit()\n    return texts\n\n\n@pytest.fixture\ndef test_annotations(db_session, test_user, test_labels, test_texts):\n    \"\"\"Create test annotations.\"\"\"\n    annotations = [\n        Annotation(\n            start_char=0,\n            end_char=10,\n            selected_text=\"John Smith\",\n            text_id=test_texts[0].id,\n            annotator_id=test_user.id,\n            label_id=test_labels[0].id,  # PERSON\n            confidence_score=0.9\n        ),\n        Annotation(\n            start_char=20,\n            end_char=29,\n            selected_text=\"Microsoft\",\n            text_id=test_texts[0].id,\n            annotator_id=test_user.id,\n            label_id=test_labels[2].id,  # ORGANIZATION\n            confidence_score=0.95\n        ),\n        Annotation(\n            start_char=33,\n            end_char=40,\n            selected_text=\"Seattle\",\n            text_id=test_texts[0].id,\n            annotator_id=test_user.id,\n            label_id=test_labels[1].id,  # LOCATION\n            confidence_score=0.85\n        )\n    ]\n    \n    for annotation in annotations:\n        db_session.add(annotation)\n    db_session.commit()\n    return annotations\n\n\nclass TestBatchProcessor:\n    \"\"\"Test batch processor functionality.\"\"\"\n    \n    @pytest.fixture\n    def processor(self):\n        return BatchProcessor(max_workers=2, chunk_size=5)\n    \n    @pytest.mark.asyncio\n    async def test_create_annotations_batch_success(self, processor, test_user, test_project, test_labels, test_texts, db_session):\n        \"\"\"Test successful batch annotation creation.\"\"\"\n        # Prepare test data\n        annotations_data = [\n            {\n                \"start_char\": 44,\n                \"end_char\": 62,\n                \"selected_text\": \"software engineer\",\n                \"text_id\": test_texts[0].id,\n                \"label_id\": test_labels[0].id,\n                \"confidence_score\": 0.8\n            },\n            {\n                \"start_char\": 0,\n                \"end_char\": 9,\n                \"selected_text\": \"Apple Inc\",\n                \"text_id\": test_texts[1].id,\n                \"label_id\": test_labels[2].id,\n                \"confidence_score\": 0.9\n            }\n        ]\n        \n        operation_id = str(uuid4())\n        \n        # Mock progress callback\n        progress_callback = AsyncMock()\n        \n        with patch.object(processor, 'session_factory') as mock_session_factory:\n            mock_session_factory.return_value = db_session\n            \n            result = await processor.create_annotations_batch(\n                operation_id=operation_id,\n                annotations_data=annotations_data,\n                user_id=test_user.id,\n                project_id=test_project.id,\n                validate_before_create=True,\n                progress_callback=progress_callback\n            )\n        \n        # Assertions\n        assert isinstance(result, BatchResult)\n        assert result.success_count == 2\n        assert result.failure_count == 0\n        assert len(result.processed_items) == 2\n        assert len(result.errors) == 0\n        assert result.execution_time > 0\n        \n        # Verify progress callback was called\n        assert progress_callback.call_count > 0\n    \n    @pytest.mark.asyncio\n    async def test_create_annotations_batch_validation_failure(self, processor, test_user, test_project):\n        \"\"\"Test batch creation with validation failures.\"\"\"\n        # Invalid data (missing required fields)\n        annotations_data = [\n            {\n                \"start_char\": 0,\n                \"end_char\": 10,\n                \"selected_text\": \"Test\",\n                # Missing text_id and label_id\n            },\n            {\n                \"start_char\": 15,  # Invalid: start >= end\n                \"end_char\": 10,\n                \"selected_text\": \"Invalid\",\n                \"text_id\": 999,  # Non-existent\n                \"label_id\": 999  # Non-existent\n            }\n        ]\n        \n        operation_id = str(uuid4())\n        \n        with patch.object(processor, 'session_factory') as mock_session_factory:\n            mock_session = Mock()\n            mock_session_factory.return_value = mock_session\n            \n            result = await processor.create_annotations_batch(\n                operation_id=operation_id,\n                annotations_data=annotations_data,\n                user_id=test_user.id,\n                project_id=test_project.id,\n                validate_before_create=True\n            )\n        \n        # Should have validation failures\n        assert result.success_count == 0\n        assert result.failure_count > 0\n        assert len(result.errors) > 0\n    \n    @pytest.mark.asyncio\n    async def test_update_annotations_batch(self, processor, test_annotations, test_user):\n        \"\"\"Test batch annotation updates.\"\"\"\n        updates_data = [\n            {\n                \"annotation_id\": test_annotations[0].id,\n                \"updates\": {\n                    \"confidence_score\": 0.95,\n                    \"notes\": \"Updated annotation\"\n                }\n            },\n            {\n                \"annotation_id\": test_annotations[1].id,\n                \"updates\": {\n                    \"confidence_score\": 0.85\n                }\n            }\n        ]\n        \n        operation_id = str(uuid4())\n        \n        with patch.object(processor, 'session_factory') as mock_session_factory:\n            mock_session = Mock()\n            mock_session.query.return_value.filter.return_value.first.side_effect = test_annotations[:2]\n            mock_session_factory.return_value = mock_session\n            \n            result = await processor.update_annotations_batch(\n                operation_id=operation_id,\n                updates_data=updates_data,\n                user_id=test_user.id\n            )\n        \n        assert isinstance(result, BatchResult)\n        assert result.success_count == 2\n        assert result.failure_count == 0\n    \n    @pytest.mark.asyncio\n    async def test_delete_annotations_batch(self, processor, test_annotations, test_user):\n        \"\"\"Test batch annotation deletion.\"\"\"\n        annotation_ids = [ann.id for ann in test_annotations[:2]]\n        operation_id = str(uuid4())\n        \n        with patch.object(processor, 'session_factory') as mock_session_factory:\n            mock_session = Mock()\n            mock_session.query.return_value.filter.return_value.first.side_effect = test_annotations[:2]\n            mock_session_factory.return_value = mock_session\n            \n            result = await processor.delete_annotations_batch(\n                operation_id=operation_id,\n                annotation_ids=annotation_ids,\n                user_id=test_user.id\n            )\n        \n        assert isinstance(result, BatchResult)\n        assert result.success_count == 2\n        assert result.failure_count == 0\n        assert len(result.processed_items) == 2\n\n\nclass TestProgressTracker:\n    \"\"\"Test progress tracking functionality.\"\"\"\n    \n    @pytest.fixture\n    def tracker(self):\n        return ProgressTracker(max_history_size=100, db_log_interval=5)\n    \n    def test_initialize_operation(self, tracker):\n        \"\"\"Test operation initialization.\"\"\"\n        operation_id = str(uuid4())\n        \n        tracker.initialize_operation(\n            operation_id=operation_id,\n            total_items=100,\n            description=\"Test operation\",\n            metadata={\"test\": \"data\"}\n        )\n        \n        progress = tracker.get_progress(operation_id)\n        assert progress[\"operation_id\"] == operation_id\n        assert progress[\"total_items\"] == 100\n        assert progress[\"current_item\"] == 0\n        assert progress[\"progress_percentage\"] == 0.0\n        assert progress[\"status\"] == \"initialized\"\n        assert progress[\"metadata\"][\"test\"] == \"data\"\n    \n    def test_update_progress(self, tracker):\n        \"\"\"Test progress updates.\"\"\"\n        operation_id = str(uuid4())\n        \n        tracker.initialize_operation(operation_id, 50, \"Test operation\")\n        \n        # Update progress\n        tracker.update_progress(\n            operation_id=operation_id,\n            current_item=25,\n            step_name=\"Processing\",\n            step_description=\"Processing items\",\n            metadata={\"processed_batch\": 1}\n        )\n        \n        progress = tracker.get_progress(operation_id)\n        assert progress[\"current_item\"] == 25\n        assert progress[\"progress_percentage\"] == 50.0\n        assert progress[\"step_name\"] == \"Processing\"\n        assert progress[\"status\"] == \"running\"\n    \n    def test_complete_operation(self, tracker):\n        \"\"\"Test operation completion.\"\"\"\n        operation_id = str(uuid4())\n        \n        tracker.initialize_operation(operation_id, 10, \"Test operation\")\n        tracker.update_progress(operation_id, 5, \"Halfway\")\n        tracker.complete_operation(operation_id, \"Successfully completed\")\n        \n        progress = tracker.get_progress(operation_id)\n        assert progress[\"status\"] == \"completed\"\n        assert progress[\"progress_percentage\"] == 100.0\n        assert progress[\"current_item\"] == 10\n        assert \"completion_time\" in progress\n    \n    def test_fail_operation(self, tracker):\n        \"\"\"Test operation failure.\"\"\"\n        operation_id = str(uuid4())\n        \n        tracker.initialize_operation(operation_id, 10, \"Test operation\")\n        tracker.fail_operation(operation_id, \"Test error message\")\n        \n        progress = tracker.get_progress(operation_id)\n        assert progress[\"status\"] == \"failed\"\n        assert progress[\"error_message\"] == \"Test error message\"\n    \n    def test_progress_history(self, tracker):\n        \"\"\"Test progress history tracking.\"\"\"\n        operation_id = str(uuid4())\n        \n        tracker.initialize_operation(operation_id, 10, \"Test operation\")\n        \n        # Multiple updates\n        for i in range(1, 6):\n            tracker.update_progress(operation_id, i, f\"Step {i}\")\n        \n        history = tracker.get_progress_history(operation_id)\n        assert len(history) > 0\n        \n        # Test limited history\n        limited_history = tracker.get_progress_history(operation_id, limit=3)\n        assert len(limited_history) <= 3\n\n\nclass TestValidationEngine:\n    \"\"\"Test validation engine functionality.\"\"\"\n    \n    @pytest.fixture\n    def engine(self):\n        return ValidationEngine()\n    \n    @pytest.mark.asyncio\n    async def test_validate_annotation_success(self, engine):\n        \"\"\"Test successful annotation validation.\"\"\"\n        annotation_data = {\n            \"start_char\": 0,\n            \"end_char\": 10,\n            \"selected_text\": \"Test text\",\n            \"text_id\": 1,\n            \"label_id\": 1,\n            \"confidence_score\": 0.9\n        }\n        \n        with patch.object(engine, 'session_factory') as mock_session_factory:\n            mock_session = Mock()\n            mock_session.query.return_value.filter.return_value.first.return_value = True\n            mock_session_factory.return_value = mock_session\n            \n            result = await engine.validate_annotation(annotation_data, project_id=1)\n        \n        assert isinstance(result, ValidationResult)\n        assert result.is_valid\n        assert result.score > 0\n        assert len(result.rules_applied) > 0\n    \n    @pytest.mark.asyncio\n    async def test_validate_annotation_failure(self, engine):\n        \"\"\"Test annotation validation with errors.\"\"\"\n        annotation_data = {\n            \"start_char\": 10,  # Invalid: start >= end\n            \"end_char\": 5,\n            \"selected_text\": \"Test\",\n            # Missing required fields\n        }\n        \n        result = await engine.validate_annotation(annotation_data)\n        \n        assert isinstance(result, ValidationResult)\n        assert not result.is_valid\n        assert len(result.issues) > 0\n        assert result.score < 1.0\n    \n    @pytest.mark.asyncio\n    async def test_batch_validate_annotations(self, engine, test_annotations):\n        \"\"\"Test batch annotation validation.\"\"\"\n        annotation_ids = [ann.id for ann in test_annotations]\n        \n        with patch.object(engine, 'session_factory') as mock_session_factory:\n            mock_session = Mock()\n            mock_session.query.return_value.filter.return_value.all.return_value = test_annotations\n            mock_session_factory.return_value = mock_session\n            \n            results = await engine.batch_validate_annotations(annotation_ids)\n        \n        assert isinstance(results, dict)\n        assert len(results) == len(annotation_ids)\n        \n        for ann_id, result in results.items():\n            assert isinstance(result, ValidationResult)\n            assert ann_id in annotation_ids\n\n\nclass TestBatchImportExport:\n    \"\"\"Test import/export functionality.\"\"\"\n    \n    @pytest.fixture\n    def import_export(self):\n        return BatchImportExport()\n    \n    def test_supported_formats(self, import_export):\n        \"\"\"Test supported formats retrieval.\"\"\"\n        formats = import_export.get_supported_formats()\n        \n        assert \"import\" in formats\n        assert \"export\" in formats\n        assert isinstance(formats[\"import\"], list)\n        assert isinstance(formats[\"export\"], list)\n        assert \"csv\" in formats[\"import\"]\n        assert \"json\" in formats[\"export\"]\n    \n    def test_parse_csv_content(self, import_export):\n        \"\"\"Test CSV content parsing.\"\"\"\n        csv_content = \"\"\"start_char,end_char,selected_text,text_id,label_id,confidence_score\n0,10,\"Test text\",1,1,0.9\n15,25,\"Another\",1,2,0.8\"\"\"\n        \n        content_bytes = csv_content.encode('utf-8')\n        data = import_export._parse_csv_content(content_bytes)\n        \n        assert len(data) == 2\n        assert data[0][\"start_char\"] == 0\n        assert data[0][\"selected_text\"] == \"Test text\"\n        assert data[1][\"confidence_score\"] == 0.8\n    \n    def test_parse_json_content(self, import_export):\n        \"\"\"Test JSON content parsing.\"\"\"\n        json_data = [\n            {\n                \"start_char\": 0,\n                \"end_char\": 10,\n                \"selected_text\": \"Test\",\n                \"text_id\": 1,\n                \"label_id\": 1\n            },\n            {\n                \"start_char\": 15,\n                \"end_char\": 25,\n                \"selected_text\": \"Another\",\n                \"text_id\": 1,\n                \"label_id\": 2\n            }\n        ]\n        \n        content_bytes = json.dumps(json_data).encode('utf-8')\n        data = import_export._parse_json_content(content_bytes)\n        \n        assert len(data) == 2\n        assert data[0][\"selected_text\"] == \"Test\"\n        assert data[1][\"text_id\"] == 1\n    \n    def test_parse_jsonl_content(self, import_export):\n        \"\"\"Test JSONL content parsing.\"\"\"\n        jsonl_content = '{\"start_char\": 0, \"end_char\": 10, \"text\": \"Test\"}\\n{\"start_char\": 15, \"end_char\": 25, \"text\": \"Another\"}'\n        \n        content_bytes = jsonl_content.encode('utf-8')\n        data = import_export._parse_jsonl_content(content_bytes)\n        \n        assert len(data) == 2\n        assert data[0][\"start_char\"] == 0\n        assert data[1][\"end_char\"] == 25\n    \n    @pytest.mark.asyncio\n    async def test_import_annotations_success(self, import_export, test_project, test_user, test_texts, test_labels):\n        \"\"\"Test successful annotation import.\"\"\"\n        # Create test data\n        import_data = [\n            {\n                \"start_char\": 44,\n                \"end_char\": 62,\n                \"selected_text\": \"software engineer\",\n                \"text_id\": test_texts[0].id,\n                \"label_id\": test_labels[0].id,\n                \"confidence_score\": 0.8\n            }\n        ]\n        \n        json_content = json.dumps(import_data).encode('utf-8')\n        \n        with patch.object(import_export, 'session_factory') as mock_session_factory:\n            mock_session = Mock()\n            mock_session.query.return_value.filter.return_value.first.return_value = True\n            mock_session_factory.return_value = mock_session\n            \n            result = await import_export.import_annotations_from_file(\n                file_content=json_content,\n                file_format=\"json\",\n                project_id=test_project.id,\n                user_id=test_user.id\n            )\n        \n        assert result.success_count > 0\n        assert result.failure_count == 0\n    \n    @pytest.mark.asyncio\n    async def test_export_annotations_csv(self, import_export, test_annotations):\n        \"\"\"Test CSV export functionality.\"\"\"\n        with tempfile.NamedTemporaryFile(mode='w+', suffix='.csv', delete=False) as f:\n            output_path = f.name\n        \n        try:\n            await import_export._export_to_csv(\n                annotations=test_annotations,\n                output_path=output_path,\n                include_metadata=True,\n                progress_callback=None\n            )\n            \n            # Verify file was created and has content\n            with open(output_path, 'r') as f:\n                content = f.read()\n                assert len(content) > 0\n                assert \"John Smith\" in content  # Test annotation text\n        \n        finally:\n            # Clean up\n            import os\n            os.unlink(output_path)\n    \n    @pytest.mark.asyncio\n    async def test_export_annotations_json(self, import_export, test_annotations):\n        \"\"\"Test JSON export functionality.\"\"\"\n        with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as f:\n            output_path = f.name\n        \n        try:\n            await import_export._export_to_json(\n                annotations=test_annotations,\n                output_path=output_path,\n                include_metadata=True,\n                progress_callback=None\n            )\n            \n            # Verify file was created and has valid JSON\n            with open(output_path, 'r') as f:\n                data = json.load(f)\n                assert \"annotations\" in data\n                assert len(data[\"annotations\"]) == len(test_annotations)\n        \n        finally:\n            # Clean up\n            import os\n            os.unlink(output_path)\n\n\nclass TestBatchAPI:\n    \"\"\"Test batch API endpoints.\"\"\"\n    \n    @pytest.fixture\n    def client(self, engine):\n        \"\"\"Create test client.\"\"\"\n        def override_get_db():\n            Session = sessionmaker(bind=engine)\n            session = Session()\n            try:\n                yield session\n            finally:\n                session.close()\n        \n        app.dependency_overrides[get_db] = override_get_db\n        \n        client = TestClient(app)\n        return client\n    \n    def test_create_batch_annotations_unauthorized(self, client):\n        \"\"\"Test batch creation without authentication.\"\"\"\n        request_data = {\n            \"project_id\": 1,\n            \"annotations\": [\n                {\n                    \"start_char\": 0,\n                    \"end_char\": 10,\n                    \"selected_text\": \"Test\",\n                    \"text_id\": 1,\n                    \"label_id\": 1\n                }\n            ]\n        }\n        \n        response = client.post(\"/api/v1/batch/annotations/create\", json=request_data)\n        assert response.status_code == 401  # Unauthorized\n    \n    def test_get_batch_operation_status_not_found(self, client):\n        \"\"\"Test getting status of non-existent operation.\"\"\"\n        operation_id = str(uuid4())\n        \n        response = client.get(f\"/api/v1/batch/operations/{operation_id}/status\")\n        assert response.status_code == 401  # Unauthorized (no auth)\n    \n    def test_list_batch_operations_unauthorized(self, client):\n        \"\"\"Test listing operations without authentication.\"\"\"\n        response = client.get(\"/api/v1/batch/operations\")\n        assert response.status_code == 401  # Unauthorized\n\n\nclass TestPerformanceAndStress:\n    \"\"\"Test performance and stress scenarios.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_large_batch_processing(self):\n        \"\"\"Test processing large batches of annotations.\"\"\"\n        processor = BatchProcessor(max_workers=4, chunk_size=100)\n        \n        # Create large dataset\n        large_dataset = []\n        for i in range(1000):\n            large_dataset.append({\n                \"start_char\": i * 10,\n                \"end_char\": i * 10 + 5,\n                \"selected_text\": f\"text_{i}\",\n                \"text_id\": (i % 10) + 1,\n                \"label_id\": (i % 3) + 1,\n                \"confidence_score\": 0.8\n            })\n        \n        operation_id = str(uuid4())\n        \n        # Mock database operations for performance testing\n        with patch.object(processor, 'session_factory') as mock_session_factory:\n            mock_session = Mock()\n            mock_session_factory.return_value = mock_session\n            \n            start_time = datetime.utcnow()\n            \n            result = await processor.create_annotations_batch(\n                operation_id=operation_id,\n                annotations_data=large_dataset,\n                user_id=1,\n                project_id=1,\n                validate_before_create=False  # Skip validation for performance\n            )\n            \n            end_time = datetime.utcnow()\n            execution_time = (end_time - start_time).total_seconds()\n        \n        # Performance assertions\n        assert execution_time < 30  # Should complete within 30 seconds\n        assert result.success_count > 0\n        assert \"items_per_second\" in result.metadata\n    \n    def test_memory_usage_tracking(self):\n        \"\"\"Test memory usage tracking during operations.\"\"\"\n        tracker = ProgressTracker()\n        operation_id = str(uuid4())\n        \n        tracker.initialize_operation(operation_id, 100, \"Memory test\")\n        \n        # Simulate memory-intensive operations\n        for i in range(10):\n            tracker.update_progress(operation_id, i * 10, f\"Step {i}\")\n        \n        history = tracker.get_progress_history(operation_id)\n        \n        # Verify memory metrics are recorded\n        assert len(history) > 0\n        assert all(\"memory_usage_mb\" in snapshot for snapshot in history)\n        assert all(snapshot[\"memory_usage_mb\"] >= 0 for snapshot in history)\n    \n    @pytest.mark.asyncio\n    async def test_concurrent_batch_operations(self):\n        \"\"\"Test multiple concurrent batch operations.\"\"\"\n        processor = BatchProcessor(max_workers=2)\n        \n        # Create multiple operations\n        operations = []\n        for i in range(3):\n            operation_id = str(uuid4())\n            dataset = [\n                {\n                    \"start_char\": j,\n                    \"end_char\": j + 5,\n                    \"selected_text\": f\"text_{i}_{j}\",\n                    \"text_id\": 1,\n                    \"label_id\": 1\n                }\n                for j in range(10)\n            ]\n            \n            operations.append({\n                \"id\": operation_id,\n                \"data\": dataset\n            })\n        \n        # Run operations concurrently\n        with patch.object(processor, 'session_factory') as mock_session_factory:\n            mock_session = Mock()\n            mock_session_factory.return_value = mock_session\n            \n            tasks = []\n            for op in operations:\n                task = processor.create_annotations_batch(\n                    operation_id=op[\"id\"],\n                    annotations_data=op[\"data\"],\n                    user_id=1,\n                    project_id=1\n                )\n                tasks.append(task)\n            \n            results = await asyncio.gather(*tasks)\n        \n        # Verify all operations completed\n        assert len(results) == 3\n        for result in results:\n            assert isinstance(result, BatchResult)\n            assert result.success_count >= 0\n\n\nclass TestErrorHandlingAndEdgeCases:\n    \"\"\"Test error handling and edge cases.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_database_connection_failure(self):\n        \"\"\"Test handling of database connection failures.\"\"\"\n        processor = BatchProcessor()\n        \n        with patch.object(processor, 'session_factory') as mock_session_factory:\n            mock_session_factory.side_effect = Exception(\"Database connection failed\")\n            \n            operation_id = str(uuid4())\n            \n            with pytest.raises(Exception) as exc_info:\n                await processor.create_annotations_batch(\n                    operation_id=operation_id,\n                    annotations_data=[{\"test\": \"data\"}],\n                    user_id=1,\n                    project_id=1\n                )\n            \n            assert \"Database connection failed\" in str(exc_info.value)\n    \n    def test_invalid_operation_id_progress_tracker(self):\n        \"\"\"Test progress tracker with invalid operation ID.\"\"\"\n        tracker = ProgressTracker()\n        \n        # Try to update progress for non-existent operation\n        tracker.update_progress(\"invalid_id\", 50)\n        \n        # Should handle gracefully without crashing\n        progress = tracker.get_progress(\"invalid_id\")\n        assert progress == {}\n    \n    @pytest.mark.asyncio\n    async def test_validation_engine_malformed_data(self):\n        \"\"\"Test validation engine with malformed data.\"\"\"\n        engine = ValidationEngine()\n        \n        # Test with completely invalid data\n        invalid_data = {\n            \"invalid_field\": \"value\",\n            \"start_char\": \"not_a_number\"\n        }\n        \n        result = await engine.validate_annotation(invalid_data)\n        \n        assert isinstance(result, ValidationResult)\n        assert not result.is_valid\n        assert len(result.issues) > 0\n    \n    def test_import_export_unsupported_format(self):\n        \"\"\"Test import/export with unsupported format.\"\"\"\n        import_export = BatchImportExport()\n        \n        assert not import_export.validate_format(\"unsupported\", \"import\")\n        assert not import_export.validate_format(\"csv\", \"invalid_operation\")\n    \n    @pytest.mark.asyncio\n    async def test_empty_batch_processing(self):\n        \"\"\"Test processing empty batches.\"\"\"\n        processor = BatchProcessor()\n        operation_id = str(uuid4())\n        \n        with patch.object(processor, 'session_factory') as mock_session_factory:\n            mock_session = Mock()\n            mock_session_factory.return_value = mock_session\n            \n            result = await processor.create_annotations_batch(\n                operation_id=operation_id,\n                annotations_data=[],  # Empty list\n                user_id=1,\n                project_id=1\n            )\n        \n        assert result.success_count == 0\n        assert result.failure_count == 0\n        assert len(result.processed_items) == 0\n        assert len(result.errors) == 0\n\n\nif __name__ == \"__main__\":\n    # Run tests with pytest\n    pytest.main([\n        __file__,\n        \"-v\",\n        \"--tb=short\",\n        \"--durations=10\"\n    ])